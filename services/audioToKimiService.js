const KimiService = require('./kimiService');

/**
 * éŸ³é¢‘è½¬æ–‡å­— + Kimiå¯¹è¯é›†æˆæœåŠ¡
 * è´Ÿè´£åè°ƒè¯­éŸ³è¯†åˆ«å’ŒAIå¯¹è¯çš„å®Œæ•´æµç¨‹
 */
class AudioToKimiService {
  constructor() {
    this.kimiService = new KimiService();
    // è¿™é‡Œå¯ä»¥é…ç½®æ‚¨çš„è¯­éŸ³è¯†åˆ«æœåŠ¡
    this.audioConfig = {
      provider: process.env.AUDIO_PROVIDER || 'default',
      language: process.env.AUDIO_LANGUAGE || 'zh-CN',
      timeout: parseInt(process.env.AUDIO_TIMEOUT) || 30000
    };
  }

  /**
   * å¤„ç†è¯­éŸ³æ–‡ä»¶å¹¶ç”ŸæˆAIå›å¤
   * @param {Buffer|File} audioData - éŸ³é¢‘æ•°æ®
   * @param {Object} options - é€‰é¡¹é…ç½®
   * @returns {Promise<Object>} å®Œæ•´çš„å¤„ç†ç»“æœ
   */
  async processVoiceToChat(audioData, options = {}) {
    try {
      console.log('ğŸ¤ å¼€å§‹è¯­éŸ³å¯¹è¯æµç¨‹...');
      
      // ç¬¬ä¸€æ­¥ï¼šè¯­éŸ³è½¬æ–‡å­—
      console.log('ğŸ“ æ­¥éª¤1: è¯­éŸ³è½¬æ–‡å­—...');
      const transcriptionResult = await this.transcribeAudio(audioData, options);
      
      if (!transcriptionResult.success) {
        return {
          success: false,
          error: 'è¯­éŸ³è¯†åˆ«å¤±è´¥: ' + transcriptionResult.error,
          step: 'transcription'
        };
      }

      const transcribedText = transcriptionResult.text;
      console.log(`âœ… è¯­éŸ³è¯†åˆ«ç»“æœ: "${transcribedText}"`);

      // ç¬¬äºŒæ­¥ï¼šå‘é€ç»™Kimi AI
      console.log('ğŸ¤– æ­¥éª¤2: AIå¯¹è¯å¤„ç†...');
      const chatResult = await this.kimiService.chatCompletion(
        transcribedText,
        options.model || 'moonshot-v1-8k'
      );

      if (!chatResult.success) {
        return {
          success: false,
          error: 'AIå¯¹è¯å¤±è´¥: ' + chatResult.error,
          step: 'chat',
          transcription: transcriptionResult
        };
      }

      console.log('âœ… AIå›å¤ç”Ÿæˆå®Œæˆ');

      // è¿”å›å®Œæ•´ç»“æœ
      return {
        success: true,
        data: {
          transcription: {
            text: transcribedText,
            confidence: transcriptionResult.confidence,
            language: transcriptionResult.language,
            duration: transcriptionResult.duration
          },
          aiResponse: {
            message: chatResult.data.message,
            usage: chatResult.data.usage,
            model: chatResult.data.model
          },
          workflow: 'voice-to-text-to-ai',
          processingTime: Date.now() - (options.startTime || Date.now())
        }
      };

    } catch (error) {
      console.error('âŒ è¯­éŸ³å¯¹è¯æµç¨‹é”™è¯¯:', error);
      return {
        success: false,
        error: error.message || 'è¯­éŸ³å¯¹è¯å¤„ç†å¤±è´¥',
        step: 'unknown'
      };
    }
  }

  /**
   * å¤šè½®è¯­éŸ³å¯¹è¯
   * @param {Buffer|File} audioData - éŸ³é¢‘æ•°æ®
   * @param {Array} conversationHistory - å¯¹è¯å†å²
   * @param {Object} options - é€‰é¡¹é…ç½®
   * @returns {Promise<Object>} å¤„ç†ç»“æœ
   */
  async processVoiceToContextualChat(audioData, conversationHistory = [], options = {}) {
    try {
      console.log('ğŸ¤ å¼€å§‹å¤šè½®è¯­éŸ³å¯¹è¯...');
      
      // ç¬¬ä¸€æ­¥ï¼šè¯­éŸ³è½¬æ–‡å­—
      const transcriptionResult = await this.transcribeAudio(audioData, options);
      
      if (!transcriptionResult.success) {
        return {
          success: false,
          error: 'è¯­éŸ³è¯†åˆ«å¤±è´¥: ' + transcriptionResult.error
        };
      }

      // ç¬¬äºŒæ­¥ï¼šæ„å»ºå¯¹è¯å†å²
      const messages = [
        ...conversationHistory,
        {
          role: 'user',
          content: transcriptionResult.text
        }
      ];

      // ç¬¬ä¸‰æ­¥ï¼šå‘é€ç»™Kimi AI
      const chatResult = await this.kimiService.chatWithHistory(
        messages,
        options.model || 'moonshot-v1-8k'
      );

      if (!chatResult.success) {
        return {
          success: false,
          error: 'AIå¯¹è¯å¤±è´¥: ' + chatResult.error,
          transcription: transcriptionResult
        };
      }

      // æ›´æ–°å¯¹è¯å†å²
      const updatedHistory = [
        ...messages,
        {
          role: 'assistant',
          content: chatResult.data.message
        }
      ];

      return {
        success: true,
        data: {
          transcription: transcriptionResult,
          aiResponse: chatResult.data,
          conversationHistory: updatedHistory,
          workflow: 'contextual-voice-chat'
        }
      };

    } catch (error) {
      console.error('âŒ å¤šè½®è¯­éŸ³å¯¹è¯é”™è¯¯:', error);
      return {
        success: false,
        error: error.message
      };
    }
  }

  /**
   * è¯­éŸ³è½¬æ–‡å­—å¤„ç†
   * è¿™é‡Œé›†æˆæ‚¨ç°æœ‰çš„ audio2text_api
   * @param {Buffer|File} audioData - éŸ³é¢‘æ•°æ®
   * @param {Object} options - é€‰é¡¹
   * @returns {Promise<Object>} è½¬å½•ç»“æœ
   */
  async transcribeAudio(audioData, options = {}) {
    try {
      // TODO: åœ¨è¿™é‡Œé›†æˆæ‚¨çš„ audio2text_api ä»£ç 
      // æ ¹æ®æ‚¨çš„å®é™…APIæ¥å£è¿›è¡Œè°ƒæ•´
      
      // ç¤ºä¾‹å®ç° - è¯·æ ¹æ®æ‚¨çš„å®é™…APIæ›¿æ¢
      const result = await this.callYourAudio2TextAPI(audioData, options);
      
      return {
        success: true,
        text: result.text || result.transcript || result.content,
        confidence: result.confidence || 0.9,
        language: result.language || options.language || 'zh-CN',
        duration: result.duration || 0,
        provider: this.audioConfig.provider
      };

    } catch (error) {
      console.error('è¯­éŸ³è½¬æ–‡å­—é”™è¯¯:', error);
      return {
        success: false,
        error: error.message || 'è¯­éŸ³è¯†åˆ«å¤±è´¥'
      };
    }
  }

  /**
   * è°ƒç”¨æ‚¨çš„éŸ³é¢‘è½¬æ–‡å­—API
   * è¯·æ ¹æ®æ‚¨çš„å®é™…APIå®ç°æ›¿æ¢æ­¤æ–¹æ³•
   */
  async callYourAudio2TextAPI(audioData, options = {}) {
    // TODO: åœ¨è¿™é‡Œæ·»åŠ æ‚¨çš„ audio2text_api è°ƒç”¨é€»è¾‘
    
    // ç¤ºä¾‹ä»£ç ç»“æ„ - è¯·æ ¹æ®æ‚¨çš„å®é™…APIè°ƒæ•´ï¼š
    
    // æ–¹å¼1: å¦‚æœæ˜¯HTTP APIè°ƒç”¨
    /*
    const FormData = require('form-data');
    const form = new FormData();
    form.append('audio', audioData, 'audio.wav');
    form.append('language', options.language || 'zh-CN');
    
    const response = await fetch('YOUR_AUDIO2TEXT_API_URL', {
      method: 'POST',
      body: form,
      headers: {
        'Authorization': `Bearer ${process.env.AUDIO_API_KEY}`
      }
    });
    
    return await response.json();
    */

    // æ–¹å¼2: å¦‚æœæ˜¯æœ¬åœ°æ¨¡å—è°ƒç”¨
    /*
    const { transcribeAudio } = require('../path/to/your/audio2text_api');
    return await transcribeAudio(audioData, options);
    */

    // æ–¹å¼3: å¦‚æœæ˜¯ç¬¬ä¸‰æ–¹æœåŠ¡
    /*
    const SpeechToTextV1 = require('ibm-watson/speech-to-text/v1');
    // æˆ–è€…å…¶ä»–æœåŠ¡çš„SDK
    */

    // ä¸´æ—¶å®ç° - è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…ä»£ç 
    throw new Error('è¯·åœ¨æ­¤å¤„é›†æˆæ‚¨çš„ audio2text_api å®ç°');
  }

  /**
   * æµå¼è¯­éŸ³å¯¹è¯å¤„ç†
   * @param {Buffer|File} audioData - éŸ³é¢‘æ•°æ®
   * @param {Object} options - é€‰é¡¹
   * @param {Function} onTranscription - è¯­éŸ³è¯†åˆ«å®Œæˆå›è°ƒ
   * @param {Function} onAIResponse - AIå›å¤ç‰‡æ®µå›è°ƒ
   * @param {Function} onComplete - å®Œæˆå›è°ƒ
   * @param {Function} onError - é”™è¯¯å›è°ƒ
   */
  async processVoiceToStreamChat(audioData, options = {}) {
    const { onTranscription, onAIResponse, onComplete, onError } = options;

    try {
      // ç¬¬ä¸€æ­¥ï¼šè¯­éŸ³è½¬æ–‡å­—
      const transcriptionResult = await this.transcribeAudio(audioData, options);
      
      if (!transcriptionResult.success) {
        onError && onError(new Error('è¯­éŸ³è¯†åˆ«å¤±è´¥: ' + transcriptionResult.error));
        return;
      }

      // é€šçŸ¥è¯­éŸ³è¯†åˆ«å®Œæˆ
      onTranscription && onTranscription(transcriptionResult);

      // ç¬¬äºŒæ­¥ï¼šæµå¼AIå¯¹è¯
      const streamResult = await this.kimiService.chatStream(transcriptionResult.text, options.model);

      if (!streamResult.success) {
        onError && onError(new Error('AIå¯¹è¯å¤±è´¥: ' + streamResult.error));
        return;
      }

      // å¤„ç†æµå¼å“åº”
      const stream = streamResult.stream;
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        if (content && onAIResponse) {
          onAIResponse(content);
        }
      }

      onComplete && onComplete({
        transcription: transcriptionResult,
        workflow: 'stream-voice-chat'
      });

    } catch (error) {
      onError && onError(error);
    }
  }

  /**
   * æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶
   * @param {Array} audioFiles - éŸ³é¢‘æ–‡ä»¶æ•°ç»„
   * @param {Object} options - é€‰é¡¹
   * @returns {Promise<Array>} å¤„ç†ç»“æœæ•°ç»„
   */
  async processBatchVoiceToChat(audioFiles, options = {}) {
    const results = [];
    
    for (let i = 0; i < audioFiles.length; i++) {
      const audioFile = audioFiles[i];
      console.log(`ğŸ”„ å¤„ç†ç¬¬ ${i + 1}/${audioFiles.length} ä¸ªéŸ³é¢‘æ–‡ä»¶...`);
      
      try {
        const result = await this.processVoiceToChat(audioFile, {
          ...options,
          startTime: Date.now()
        });
        
        results.push({
          index: i,
          filename: audioFile.filename || `audio_${i}`,
          result
        });
        
      } catch (error) {
        results.push({
          index: i,
          filename: audioFile.filename || `audio_${i}`,
          result: {
            success: false,
            error: error.message
          }
        });
      }
    }
    
    return results;
  }

  /**
   * è·å–æœåŠ¡çŠ¶æ€å’Œé…ç½®ä¿¡æ¯
   * @returns {Object} æœåŠ¡çŠ¶æ€
   */
  getServiceStatus() {
    return {
      audioConfig: this.audioConfig,
      kimiService: 'initialized',
      supportedLanguages: ['zh-CN', 'en-US', 'ja-JP'],
      maxAudioSize: '50MB',
      supportedFormats: ['mp3', 'wav', 'm4a', 'flac', 'ogg']
    };
  }
}

module.exports = AudioToKimiService;